---
mindmap-plugin: basic
tags:
  - 神经计算
  - 机器学习
---

# 线性回归

## 问题设置
- 数据集
Dataset ($D$)
	- $n$个输入输出对
	经验
	Experience $E$
		- $$
		D = \{ (\mathbf{x}^{(1)}, y^{(1)}),(\mathbf{x}^{(2)}, y^{(2)})\dots(\mathbf{x}^{(n)}, y^{(n)}) \}
		$$
- 回归任务
Regression Task (T)
	- 找出一个模型
	$$f(\mathbf{x}) \ , f: \mathbb{R}^d \to \mathbb{R}$$
		- 使得$f(\mathbf{x})$的输出接近真实输出$y$
- 线性模型
void
Linear Model
	- 形如:
	$$
	f(\mathbf{x}) = w_0 + w_1x_1 + \dots + w_dx_d
	$$
		- 偏移量
		bias (intercept)
			- $w_0$
		- 权重
		weight (parameters)
			- $w_1,w_2, \dots , w_d$
		- 特征
		feature
			- $x_i$是向量$\mathbf{x} \in \mathbb{R}^d$的第i个元素

## 线性回归
- 概念
	- 求偏差小的线性函数
- 公式
	- $$\begin{align}
	f(\mathbf{x})  & = w_{0} + w_{1}x_{1} + \dots + w_{d}x_{d} \\
	& = (w_{0} + w_{1} + \dots + w_{d}) \left( \begin{matrix}
	1 \\ x
	\end{matrix} \right) \\
	& = \mathbf{w}^T \bar{\mathbf{x}}
	\end{align}$$
		- 化简
			- $$f(\mathbf{x})=\mathbf{w}^T\mathbf{x}$$

## 性能衡量
- 均方误差
Mean Square Error(MSE)
	- 第i组数据的残差
	"residual"
		- $$\begin{align}
		e^{(i)} & =y^{(i)}-\mathbf{w}^T \mathbf{x}^{(i)} \\
		& =y^{(i)}- \mathbf{x}^{(i)^T} \mathbf{w}^T
		\end{align}$$
	- MSE计算全部n组数据的残差
		- $$\begin{align}
		C(\mathbf{w}) & = \frac{1}{2n}\sum_{i=1}^n(y^{(i)}-x^{(i)^T}\mathbf{w})^2 \\
		& = \frac{1}{2n}\sum_{i=1}^n(e^{(i)})^2
		\end{align}
		$$
	- 对残差(residual) $e$ 平方的效果
		- 忽略了残差的符号
		- 加大对大额残差的惩罚力度
			- (若$e^{(i)}>1$)
	- 目标
		- 寻找参数$\mathbf(w)$使得$C(\mathbf{w})$最小
- 最小二乘法
least square
	- 概念
		- 试图找到一条直线
			- 使得所有样本到直线的欧几里得距离之和最小
	- 一维情况
		- $$\begin{align}
		C(w) & =\frac{1}{2n}\sum_{i=1}^{n}(y^{(i)}-x^{(i)}w)^2 \\
		& =\frac{1}{2n}\sum_{i=1}^n(x^{(i)^2}w^2-2y^{(i)}x^{(i)}w+y^{(i)^2})
		\end{align}$$
		- $$\begin{align}
		C'(w) & =\frac{1}{2n}\sum_{i=1}^{n}(2x^{(i)^2}w-2y^{(i)}x^{(i)}) \\
		& =\frac{1}{n}\sum_{i=1}^{n}x^{(i)^2}w -\frac{1}{n}\sum_{i=1}^{n}y^{(i)}x^{(i)}
		\end{align}$$
		- 根据一阶优化条件，
		最优解$w^*$满足:
			- $$\begin{align}
			C'(w) & =0 \\
			\frac{1}{n}\sum_{i=1}^{n}x^{(i)^2}w^* & = \frac{1}{n}\sum_{i=1}^{n}y^{(i)}x^{(i)} \\
			w^* & = \frac{\sum_{i=1}^{n}y^{(i)}x^{(i)}}{\sum_{i=1}^{n}x^{(i)^2}}
			\end{align}$$
	- 矩阵情况
		- $$
		C(\mathbf{w})=\frac{1}{2n}\sum_{i=1}^{n}(y^{(i)}-\mathbf{x}^{(i)^T}\mathbf{w}^{(i)})$$
		- $$\mathbf{x}^{(i)^T}=(x_{1}^{(i)},x_{2}^{(i)}\dots x_{d}^{(i)})$$
			- $$\begin{align}
			X  & = \left( \begin{matrix}
			\mathbf{x}^{(1)^T} \\
			\vdots \\
			\mathbf{x}^{(n)^T}
			\end{matrix} \right) \in \mathbb{R}^{n\times d} \\
			\mathbf{y} & =\left( \begin{matrix}
			y^{(1)} \\
			\vdots \\
			y^{(n)}
			\end{matrix} \right) \in \mathbb{R}^n
			\end{align}$$
				- $$X\mathbf{w} - \mathbf{y} = \left( \begin{matrix}
				\mathbf{x}^{(1)^T}\mathbf{w}-y^{(1)} \\
				\vdots \\
				\mathbf{x}^{(n)^T}\mathbf{w}-y^{(n)}
				\end{matrix} \right) $$
		- $$\begin{align}
		(X\mathbf{w} - \mathbf{y})^T(X\mathbf{w} - \mathbf{y}) & = \left( \mathbf{x}^{(1)^T}\mathbf{w}-y^{(1)} \ \ \dots \ \ \mathbf{x}^{(n)^T}\mathbf{w}-y^{(n)} \right) \left( \begin{matrix}
		\mathbf{x}^{(1)^T}\mathbf{w}-y^{(1)} \\
		\vdots \\
		\mathbf{x}^{(n)^T}\mathbf{w}-y^{(n)}
		\end{matrix} \right) \\
		& = \sum_{i=1}^n(\mathbf{x}^{(i)}\mathbf{w} - y^{(i)})^2
		\end{align}$$
		- $$\begin{align}
		C(\mathbf{w}) & =\frac{1}{2n}(X\mathbf{w} - \mathbf{y})^T(X\mathbf{w} - \mathbf{y}) \\
		& =\frac{1}{2n}(\mathbf{w}^TX^T-\mathbf{y}^T)(X\mathbf{w} - \mathbf{y}) \\
		& =\frac{1}{2n}(\mathbf{w}^TX^TX\mathbf{w} - 2\mathbf{w}^TX^T\mathbf{y} + \mathbf{y}^T\mathbf{y})
		\end{align}$$
		- 偏导公式
			- $$
			\frac{\partial(\mathbf{x}^TA\mathbf{x})}{\partial \mathbf{x}}=A\mathbf{x}+A^T\mathbf{x}
			$$ ^f6ea647a-0ace-2c7e
				- 证明[[W1(2)]]
			- $$
			\frac{\partial (\mathbf{x}^T\mathbf{a})}{\partial \mathbf{x}} = \frac{\partial (\mathbf{a}^T\mathbf{x})}{\partial \mathbf{x}} = \mathbf{a}
			$$
		- $$\begin{align}
		\nabla C(\mathbf{w}) & =\frac{1}{2n}(2X^TX\mathbf{w}-2X^T\mathbf{y}) \\
		& =\frac{1}{n}(X^TX\mathbf{w}-X^T\mathbf{y})
		\end{align}$$
			- $$\begin{align}
			\nabla C(\mathbf{w}^*) & \Longrightarrow \frac{1}{n}(X^TX\mathbf{w}^*-X^T\mathbf{y})  = 0 \\
			& \Longrightarrow X^TX\mathbf{w}^* = X^T\mathbf{y}
			\end{align}$$
				- 若$X^TX$可逆则:
				$$
				\mathbf{w}^* = (X^TX)^{-1}X^T\mathbf{y}
				$$
		- 因此:
		$$\begin{align}
		\mathbf{w}^* & = (X^TX)^{-1}X^T\mathbf{y} \\ \\
		\hat{\mathbf{y}} & =X\mathbf{w}^* \\
		\end{align}$$